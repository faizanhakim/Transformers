{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43d19329-a5d8-4165-bd2c-a16383144b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22163e8c-a9c8-44fb-8e6d-d9fec90f31d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "L, d_k, d_v = 4, 8, 8\n",
    "q = np.random.randn(L, d_k)\n",
    "k = np.random.randn(L, d_k)\n",
    "v = np.random.randn(L, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc695df0-1d28-4b30-aba2-adab044fad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    d_k = q.shape[-1]\n",
    "    scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled = scaled + mask\n",
    "    attention = softmax(scaled)\n",
    "    out = np.matmul(attention, v)\n",
    "    return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "249c5dbc-9caa-40a1-b853-7428dbea8d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, attention = scaled_dot_product_attention(q, k, v, mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "014316e7-406b-4e26-84a3-0312414a0b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V\n",
      " [[-0.97166797 -0.72445974 -0.14492141 -0.36504448  0.39600001 -1.29059403\n",
      "   1.55400334 -0.07725151]\n",
      " [ 0.9703225   0.20320333  0.00597748 -0.65981981  0.97693662 -0.04148718\n",
      "   2.76405843 -0.58349439]\n",
      " [-0.23541647 -0.12086961 -0.52406952 -0.68212544 -0.67754271 -0.23636492\n",
      "   0.67179366 -0.2122306 ]\n",
      " [ 0.8985193   0.83856752 -0.88650342 -0.84329342  1.03686493 -1.0993069\n",
      "  -0.53747527  0.09869986]]\n",
      "Attention\n",
      " [[0.72516476 0.14096337 0.06004716 0.07382471]\n",
      " [0.05840297 0.40937212 0.13249747 0.39972743]\n",
      " [0.38917516 0.19027227 0.289825   0.13072756]\n",
      " [0.41491756 0.2373883  0.16562815 0.18206598]]\n"
     ]
    }
   ],
   "source": [
    "print(\"V\\n\", v)\n",
    "print(\"Attention\\n\", attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
